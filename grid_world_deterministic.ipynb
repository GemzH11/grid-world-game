{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f1076f-30c3-4242-912c-7d82d97ef875",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grid World Game\n",
    "Based on the tutorial [here](https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff) by Jeremy Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae8fb77-7328-4134-8a36-88d43bf95b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Global Variables\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (3, 0)\n",
    "LOSE_STATE = (3, 1)\n",
    "START = (0, 0)\n",
    "WALLS = [(1, 1)]\n",
    "DETERMINISTIC = True\n",
    "LEARNING_RATE = 0.2\n",
    "EXPLORATION_RATE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92dcb9c-ca3a-48ee-8de3-d9da15a3c135",
   "metadata": {},
   "source": [
    "## Rules\n",
    "![The grid](GridWorldGrid.png)\n",
    " - The agent starts at the bottom-left corner and ends at either +1 or -1 (the reward). \n",
    " - There are 4 possible actions (up, down, left, right).\n",
    " - If the agent hits the wall, it will remain in the same position.\n",
    "\n",
    "Note that this first attempt is deterministic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d975dd-c9b9-4fff-8014-b03cbce73c3b",
   "metadata": {},
   "source": [
    "## Board\n",
    "For each iteration we need a way to return the reward given a particular state, take in the current state and action and return the next state, and a way to know when the current iteration is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc39840-f493-4c63-b00c-d8d4c361f164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Iteration:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1,1] = -1\n",
    "        self.state = state\n",
    "        self.is_end = False\n",
    "\n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def next_position(self, action):\n",
    "        \"\"\"\n",
    "        2 |\n",
    "        1 |\n",
    "        0 | 1 | 2 | 3\n",
    "        \"\"\"\n",
    "        if DETERMINISTIC:\n",
    "            if action == \"up\":\n",
    "                next_state = (self.state[0], self.state[1] + 1)\n",
    "            elif action == \"down\":\n",
    "                next_state = (self.state[0], self.state[1] - 1)\n",
    "            elif action == \"left\":\n",
    "                next_state = (self.state[0] - 1, self.state[1])\n",
    "            else:\n",
    "                next_state = (self.state[0] + 1, self.state[1])\n",
    "                \n",
    "        # Check that the next state is legal\n",
    "        if (next_state[0] >= 0) and (next_state[0] <= 3):\n",
    "            if (next_state[1] >= 0) and (next_state[1] <= 2):\n",
    "                if next_state not in WALLS:\n",
    "                    self.state = next_state\n",
    "        return self.state\n",
    "\n",
    "    def is_end_func(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.is_end = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecb559-d9ec-47d1-b3c4-e5e4e73abbc6",
   "metadata": {},
   "source": [
    "## Agent\n",
    "### Value Iteration\n",
    "The agent will learn a policy (a mapping from state to action), that instructs what the agent should do at each state.\n",
    "\n",
    "Instead here we will use valie iteration to first learn a mapping of state to value (estimated reward), i.e. how much is each cell worth in reward? Based on the estimation at each state the agnet will choose the best action that gives the highest estimated reward.\n",
    "\n",
    "Value iteration formula: $${\\large V(S_t) \\leftarrow V(S_t)+ \\alpha \\left[ V(S_{t+1}) - V(S_t) \\right]}$$\n",
    "\n",
    "As the name suggests, the value is updated at the end of each iteration, working backwards through the path that the agent took. When the agent first starts, all cells have value 0 (except the start/end). Once the agent reaches the end of the game, the reward propagates in a backwards fashion so the estimated value of all states along the way will be updated using the formula.\n",
    "\n",
    "$V(S_t)$ on the left is the updated value of the state. on the right of the arrow $V(S_t)$ is the current value of the state, $ \\alpha $ is the learning rate and $V(S_{t+1})$ is the value of the next state of the agent. The formula says that the updated value of a state is equal to the current value plus a temporal difference (what the agent learned from this iteration minus the previous estimate).\n",
    "\n",
    "We need our agent to be able to 'play' the game. Once an iteration is finished, we need to be able to reset our agent ready for the next iteration. It would also be useful to have a helper method that displays the values at the end of each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b2927-ed15-4c8b-9ba7-2bdfb94362ac",
   "metadata": {},
   "source": [
    "### Exploration vs. Expoitation\n",
    "Once the agent finds a path to get reward +1, should it stick to it foreger or give another path a chance in hope of finding a shorter one? This needs to be balanced to avoid the agent getting stuck in a local optimal. The action will be chosen based on a certain exploration rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a67e72a-14ed-4c10-ae2a-045a3646df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.iteration = Iteration()\n",
    "        self.is_end = self.iteration.is_end\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.exploration_rate = EXPLORATION_RATE\n",
    "\n",
    "        # Set up an initial empty set of rewards\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_COLS):\n",
    "            for j in range(BOARD_ROWS):\n",
    "                self.state_values[(i,j)] = 0\n",
    "\n",
    "    def take_action(self, action):\n",
    "        position = self.iteration.next_position(action)\n",
    "        return Iteration(state=position)\n",
    "\n",
    "    def play(self, rounds = 10):\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            # to the end of the game back-propagate the reward\n",
    "            if self.iteration.is_end:\n",
    "                # back propagate\n",
    "                reward = self.iteration.giveReward()\n",
    "                # explicitly assign end state to reward values\n",
    "                self.state_values[self.iteration.state] = reward\n",
    "                print(\"Game end reward\", reward)\n",
    "                for s in reversed(self.states):\n",
    "                    reward = self.state_values[s] + self.learning_rate * (reward - self.state_values[s])\n",
    "                    self.state_values[s] = round(reward, 3)\n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.choose_action()\n",
    "                # append trace\n",
    "                self.states.append(self.iteration.next_position(action))\n",
    "                #print(\"current position {} action {}\".format(self.iteration.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.iteration = self.take_action(action)\n",
    "                # check whether we have reached the end\n",
    "                self.iteration.is_end_func()\n",
    "                #print(\"next state\", self.iteration.state)\n",
    "                #print(\"--------------------\")\n",
    "\n",
    "    def choose_action(self):\n",
    "        # choose the action with the most expected value\n",
    "        max_next_reward = 0\n",
    "        action = \"\"\n",
    "        if np.random.uniform(0,1) <= self.exploration_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                # if the action is deterministic\n",
    "                next_reward = self.state_values[self.iteration.next_position(a)]\n",
    "                if next_reward > max_next_reward:\n",
    "                    action = a\n",
    "                    max_next_reward = next_reward\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.show_values()\n",
    "        self.states = []\n",
    "        self.iteration = Iteration()\n",
    "        self.is_end = self.iteration.is_end\n",
    "\n",
    "    def show_values(self):\n",
    "        for i in range(0 , BOARD_ROWS):\n",
    "            print(\"-------------------------\")\n",
    "            out = \" | \"\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.state_values[(j,i)]) + \" | \"\n",
    "            print(out)\n",
    "        print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977ce54-3a40-4592-9e7e-3dfeb3f81bce",
   "metadata": {},
   "source": [
    "Finally, we can initialise our agent and play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ede3fe-5823-4248-8857-d355ba1c87ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0 | 0 | 0.2 | 1 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.072 | 0 | 0.36 | 1 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.078 | 0.08 | 0.488 | 1 | \n",
      "-------------------------\n",
      " | 0.015 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.078 | 0.08 | 0.59 | 1 | \n",
      "-------------------------\n",
      " | 0.015 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.197 | 0.08 | 0.672 | 1 | \n",
      "-------------------------\n",
      " | 0.051 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.204 | 0.206 | 0.738 | 1 | \n",
      "-------------------------\n",
      " | 0.051 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.204 | 0.206 | 0.79 | 1 | \n",
      "-------------------------\n",
      " | 0.051 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.236 | 0.365 | 0.79 | 1.0 | \n",
      "-------------------------\n",
      " | 0.05 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0.047 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.236 | 0.365 | 0.832 | 1 | \n",
      "-------------------------\n",
      " | 0.05 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0.047 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.236 | 0.365 | 0.866 | 1 | \n",
      "-------------------------\n",
      " | 0.05 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0.047 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.236 | 0.365 | 0.893 | 1 | \n",
      "-------------------------\n",
      " | 0.05 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0.047 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.372 | 0.365 | 0.914 | 1 | \n",
      "-------------------------\n",
      " | 0.05 | 0 | 0 | 0 | \n",
      "-------------------------\n",
      " | 0.047 | 0 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward -1\n",
      "-------------------------\n",
      " | 0.281 | 0.321 | 0.914 | 1 | \n",
      "-------------------------\n",
      " | 0.065 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | 0.047 | -0.2 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.411 | 0.321 | 0.931 | 1 | \n",
      "-------------------------\n",
      " | 0.065 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | 0.047 | -0.2 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward -1\n",
      "-------------------------\n",
      " | 0.324 | 0.322 | 0.931 | 1 | \n",
      "-------------------------\n",
      " | -0.02 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | 0.047 | -0.36 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward -1\n",
      "-------------------------\n",
      " | 0.336 | 0.347 | 0.802 | 1 | \n",
      "-------------------------\n",
      " | -0.028 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.336 | 0.347 | 0.842 | 1 | \n",
      "-------------------------\n",
      " | -0.028 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.336 | 0.347 | 0.874 | 1 | \n",
      "-------------------------\n",
      " | -0.028 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.449 | 0.347 | 0.899 | 1 | \n",
      "-------------------------\n",
      " | -0.028 | 0 | 0 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward -1\n",
      "-------------------------\n",
      " | 0.434 | 0.425 | 0.679 | 1 | \n",
      "-------------------------\n",
      " | -0.028 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.682 | 1.0 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.746 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.797 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.838 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.87 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.896 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.917 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.934 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.947 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.958 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.966 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.459 | 0.973 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.2 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward -1\n",
      "-------------------------\n",
      " | 0.436 | 0.508 | 0.706 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.436 | 0.508 | 0.765 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.511 | 0.508 | 0.812 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.541 | 0.552 | 0.85 | 1.0 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.541 | 0.552 | 0.88 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.614 | 0.552 | 0.904 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.629 | 0.631 | 0.904 | 1.0 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.629 | 0.726 | 0.876 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.629 | 0.726 | 0.901 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.687 | 0.726 | 0.921 | 1 | \n",
      "-------------------------\n",
      " | 0.067 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.737 | 0.726 | 0.937 | 1 | \n",
      "-------------------------\n",
      " | 0.201 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.751 | 0.76 | 0.95 | 1 | \n",
      "-------------------------\n",
      " | 0.201 | 0 | -0.36 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward -1\n",
      "-------------------------\n",
      " | 0.751 | 0.74 | 0.662 | 1 | \n",
      "-------------------------\n",
      " | 0.201 | 0 | -0.488 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.744 | 0.742 | 0.73 | 1 | \n",
      "-------------------------\n",
      " | 0.201 | 0 | -0.488 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.746 | 0.748 | 0.784 | 1 | \n",
      "-------------------------\n",
      " | 0.201 | 0 | -0.488 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.762 | 0.748 | 0.827 | 1 | \n",
      "-------------------------\n",
      " | 0.201 | 0 | -0.488 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.712 | 0.741 | 0.844 | 1 | \n",
      "-------------------------\n",
      " | 0.314 | 0 | -0.488 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n",
      "Game end reward 1\n",
      "-------------------------\n",
      " | 0.712 | 0.768 | 0.875 | 1.0 | \n",
      "-------------------------\n",
      " | 0.314 | 0 | -0.488 | -1.0 | \n",
      "-------------------------\n",
      " | -0.06 | -0.488 | 0 | 0 | \n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.play(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5caa779-ef25-4930-a80e-64baf65f3ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
